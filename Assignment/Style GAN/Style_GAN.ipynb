{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wNJcuueLQFQM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import (Input, Dense, Reshape, Flatten, LeakyReLU,\n",
        "                                     Conv2D, UpSampling2D, Dropout, LayerNormalization,\n",
        "                                     multiply)\n",
        "from tensorflow.keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train, _), (_, _) = mnist.load_data()\n",
        "X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
        "X_train = np.expand_dims(X_train, axis=-1)  # (28,28,1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ETV81-5NQJhF",
        "outputId": "489671ef-65cd-4edb-ad73-74a4b1abc926"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_mapping(latent_dim=100, dlatent_dim=100):\n",
        "    z = Input(shape=(latent_dim,))\n",
        "    x = Dense(128, activation=\"relu\")(z)\n",
        "    x = Dense(dlatent_dim)(x)\n",
        "    return Model(z, x, name=\"MappingNetwork\")"
      ],
      "metadata": {
        "id": "INy4PJ75QTxw"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_generator(latent_dim=100, dlatent_dim=100):\n",
        "    style_input = Input(shape=(dlatent_dim,))\n",
        "    x = Dense(7*7*128)(style_input)\n",
        "    x = Reshape((7,7,128))(x)\n",
        "\n",
        "    # Style modulation (simplified)\n",
        "    for filters in [128, 64]:\n",
        "        x = UpSampling2D()(x)\n",
        "        x = Conv2D(filters, kernel_size=3, padding=\"same\")(x)\n",
        "        x = LayerNormalization()(x)\n",
        "        x = LeakyReLU(0.2)(x)\n",
        "        style = Dense(filters, activation=\"linear\")(style_input)\n",
        "        style = Reshape((1,1,filters))(style)\n",
        "        x = multiply([x, style])\n",
        "\n",
        "    img = Conv2D(1, kernel_size=3, padding=\"same\", activation=\"tanh\")(x)\n",
        "    return Model(style_input, img, name=\"Generator\")"
      ],
      "metadata": {
        "id": "IwpDVf5jQYA2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_discriminator(img_shape=(28,28,1)):\n",
        "    img = Input(shape=img_shape)\n",
        "    x = Conv2D(64, kernel_size=3, strides=2, padding=\"same\")(img)\n",
        "    x = LeakyReLU(0.2)(x)\n",
        "    x = Dropout(0.25)(x)\n",
        "    x = Conv2D(128, kernel_size=3, strides=2, padding=\"same\")(x)\n",
        "    x = LeakyReLU(0.2)(x)\n",
        "    x = Dropout(0.25)(x)\n",
        "    x = Flatten()(x)\n",
        "    validity = Dense(1, activation=\"sigmoid\")(x)\n",
        "    return Model(img, validity, name=\"Discriminator\")"
      ],
      "metadata": {
        "id": "KV2sgRi2Qaz9"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "latent_dim = 100\n",
        "dlatent_dim = 100\n",
        "optimizer = Adam(0.0002, 0.5)\n",
        "\n",
        "mapping = build_mapping(latent_dim, dlatent_dim)\n",
        "generator = build_generator(latent_dim, dlatent_dim)\n",
        "discriminator = build_discriminator()\n",
        "\n",
        "discriminator.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "82hScuHfQdjG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "z = Input(shape=(latent_dim,))\n",
        "w = mapping(z)\n",
        "img = generator(w)\n",
        "discriminator.trainable = False\n",
        "validity = discriminator(img)\n",
        "gan = Model(z, validity)\n",
        "gan.compile(loss=\"binary_crossentropy\", optimizer=optimizer)"
      ],
      "metadata": {
        "id": "Oq8MhENsQgeH"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epochs=10000, batch_size=64, save_interval=1000):\n",
        "    d_losses, g_losses = [], []\n",
        "\n",
        "    valid = np.ones((batch_size,1))\n",
        "    fake = np.zeros((batch_size,1))\n",
        "\n",
        "    for epoch in range(1, epochs+1):\n",
        "        # ---------------------\n",
        "        # Train Discriminator\n",
        "        # ---------------------\n",
        "        idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "        real_imgs = X_train[idx]\n",
        "\n",
        "        noise = np.random.normal(0,1,(batch_size, latent_dim))\n",
        "        w = mapping.predict(noise, verbose=0)\n",
        "        gen_imgs = generator.predict(w, verbose=0)\n",
        "\n",
        "        d_loss_real = discriminator.train_on_batch(real_imgs, valid)\n",
        "        d_loss_fake = discriminator.train_on_batch(gen_imgs, fake)\n",
        "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "        # ---------------------\n",
        "        # Train Generator\n",
        "        # ---------------------\n",
        "        noise = np.random.normal(0,1,(batch_size, latent_dim))\n",
        "        g_loss = gan.train_on_batch(noise, valid)\n",
        "\n",
        "        # Save losses\n",
        "        d_losses.append(d_loss[0])\n",
        "        g_losses.append(g_loss)\n",
        "\n",
        "        # Print progress\n",
        "        if epoch % 100 == 0:\n",
        "            print(f\"{epoch} [D loss: {d_loss[0]:.4f}, acc: {100*d_loss[1]:.2f}] [G loss: {g_loss:.4f}]\")\n",
        "\n",
        "        # Save images\n",
        "        if epoch % save_interval == 0:\n",
        "            save_imgs(epoch)\n",
        "\n",
        "    return d_losses, g_losses"
      ],
      "metadata": {
        "id": "lsnacRloQjJ-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_imgs(epoch, examples=25):\n",
        "    noise = np.random.normal(0,1,(examples, latent_dim))\n",
        "    w = mapping.predict(noise, verbose=0)\n",
        "    gen_imgs = generator.predict(w, verbose=0)\n",
        "\n",
        "    gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "\n",
        "    plt.figure(figsize=(5,5))\n",
        "    for i in range(examples):\n",
        "        plt.subplot(5,5,i+1)\n",
        "        plt.imshow(gen_imgs[i,:,:,0], cmap=\"gray\")\n",
        "        plt.axis(\"off\")\n",
        "    plt.suptitle(f\"Epoch {epoch}\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "hK9jSxb1QmkG"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d_losses, g_losses = train(epochs=5000, batch_size=64, save_interval=1000)\n",
        "\n",
        "# Plot losses\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(d_losses, label=\"Discriminator Loss\")\n",
        "plt.plot(g_losses, label=\"Generator Loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.title(\"Training Losses\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFd5AEhkQotD",
        "outputId": "1db4ccbf-631f-4c99-bd0b-50de84cd8268"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/trainer.py:83: UserWarning: The model does not have any trainable weights.\n",
            "  warnings.warn(\"The model does not have any trainable weights.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100 [D loss: 1.0395, acc: 12.07] [G loss: 0.3167]\n",
            "200 [D loss: 1.0980, acc: 11.91] [G loss: 0.2734]\n"
          ]
        }
      ]
    }
  ]
}